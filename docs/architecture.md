# docs/architecture.md

## 1. 전체 개요
이 프로젝트는 “기준 문서(지침서/모의논리/매뉴얼)”를 근거로 답변하는 로컬 RAG 챗봇입니다. 핵심 흐름은 다음과 같습니다.

1) 문서 수집/파싱 → 2) 청킹/메타데이터 부여 → 3) 인덱싱(BM25/벡터) →  
4) 질의 시 하이브리드 검색 → 5) 리랭킹(선택) → 6) 근거 기반 답변 생성(인용 포함)

## 2. 데이터 모델(문서/청크)
- 문서(Document)
  - `doc_id`: 문서 식별자
- 청크(Chunk)
  - `section_path`: 문서 내 섹션 경로(헤딩 계층 기반)
  - `heading`: 문서가 포함된 바로 상위 경로
  - `chunk_id` 와 `chunk_index`: 섹션 내 청크 식별
  - `page_start`, `page_end`: 근거 페이지 범위
  - `text`: 청크 본문
  - `chunk_tokens`: 토큰 길이(길이 기반 제어/로깅/품질 지표용)

## 3. 인제스트(ingest) 파이프라인
### 3.1 문서 파싱
- PDF: PyMuPDF 기반 텍스트 추출을 기본으로 합니다.

### 3.2 청킹 전략
- 기본은 **heading-aware 청킹**(헤딩/목차를 기준으로 섹션 경로를 유지).
- 필요 시 섹션 길이에 따라:
  - 섹션 내부를 고정 길이/문장 단위로 추가 분할
  - 계층(hierarchical) 청킹을 적용할 수 있도록 확장 가능하게 설계

### 3.3 메타데이터 부여
- 각 청크는 최소한 `doc_id`, `section_path`, `heading`, `chunk_id(+ index)`, `page_start/end`, `chunk_tokens`를 가집니다.
- 섹션 경로는 검색 및 답변 인용에 직접 사용됩니다.

## 4. 인덱싱(index)
### 4.1 벡터 인덱스
- 임베딩 모델로 청크를 벡터화한 뒤 FAISS(기본)로 인덱싱합니다.
- (윈도우/환경 이슈 시) 대체로 hnswlib 같은 옵션을 고려합니다.

### 4.2 키워드 인덱스(BM25)
- `rank-bm25`로 청크 텍스트를 색인합니다.
- 한글 토큰화/전처리는 “과도한 의존성 추가 없이” 시작하고, 필요 시 개선합니다.

## 5. 검색(retrieval) 및 리랭킹(rerank)
### 5.1 하이브리드 검색
- 1차 후보를 다음 중 하나로 구성합니다.
  - (A) 벡터 Top-K + BM25 Top-K를 합쳐 후보 풀 생성
  - (B) 가중 합 스코어로 통합 랭킹(대안 있으면 대안으로)
- 후보 풀은 중복 제거 후 상위 N개만 리랭커/LLM에 전달합니다.

### 5.2 리랭킹
- CPU 기반 크로스 인코더 리랭커를 사용해 리랭킹 합니다.
- 리랭커가 꺼져 있어도 품질이 유지되도록, 기본 검색 품질(청킹/전처리/Top-K)을 우선 최적화합니다.

## 6. 답변 생성(answering)
- LLM은 llama.cpp 기반 로컬 추론을 우선합니다(오프라인 운영).
- 프롬프트는 다음을 강제합니다.
  - 제공된 근거 청크 밖의 사실을 단정하지 않기
  - 답변에 인용(문서/섹션/페이지/청크)을 포함하기
  - 근거 부족 시 “확인 불가”로 처리하기

## 7. API(예: FastAPI)
- 최소 엔드포인트 예시(구현 방식은 현재 코드 구조에 맞춤)
  - `POST /chat`: 사용자 질문 → 답변(+인용)
  - `POST /ingest`: 문서 등록/재색인(운영 환경에서는 제한 가능)
  - `GET /health`: 상태 확인

## 8. 설정(config) 및 운영
- 오프라인을 전제로, 아래 항목은 설정 파일/환경 변수로 고정 가능해야 합니다.
  - 모델 경로, 인덱스 저장 경로, 캐시 경로(HF_HOME 등), Top-K, 리랭커 on/off, 최대 컨텍스트 길이
- 실행/재색인/백업 절차를 운영자가 따라할 수 있도록 단순화합니다.
